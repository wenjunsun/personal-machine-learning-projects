{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autograd.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPeSYAIprIJDiglDQ0hDS28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wenjunsun/personal-machine-learning-projects/blob/master/deep-learning/autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6StbY4YeJTTN"
      },
      "source": [
        "in a previous notebook we manually wrote our own gradient descent algorithm, with the simplest case when model is linear and cost is MSE. What if we have a billion parameters with complex networks? We could theoretically do the same calculus chain rule and write our update rule again, but it will be slow and error prone.\r\n",
        "\r\n",
        "Now that we know what is going on under the hood, we can use Pytorch's autograd function to automatically calculate gradient on the go when we are calculating the loss. We do this by setting the parameter tensor to have`require_grad` to be true. This basically says any tensor that is calculated from this tensor Pytorch will keep an eye on them and when we calculated everything, the gradient will be available ay any intermediate tensor and the base tensor for doing gradient descent.\r\n",
        "\r\n",
        "Let's do the same thing we did before, but now with `autograd` functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCyXwiJ3Ksgx"
      },
      "source": [
        "# 1. create synthetic data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcBGvuBYIomq"
      },
      "source": [
        "import torch\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q6Is7BoKV3V"
      },
      "source": [
        "tensor_celsius = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0])\r\n",
        "tensor_fahrenheit = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "woFs17UMKczF",
        "outputId": "fa63895f-7b50-49be-a290-64b3bbbcac26"
      },
      "source": [
        "plt.scatter(tensor_celsius, tensor_fahrenheit)\r\n",
        "plt.xlabel(\"Celsius\")\r\n",
        "plt.ylabel('fahrenheit')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'fahrenheit')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m60CIt-wKuHx"
      },
      "source": [
        "# 2. define model + loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpILZBxKKnFd"
      },
      "source": [
        "def model(input_tensor, w, b):\r\n",
        "  return input_tensor * w + b"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pxchpVvK08v"
      },
      "source": [
        "# expects two tesnors, return mean square error.\r\n",
        "def loss(predicted, expected):\r\n",
        "  square_diffs = (predicted - expected)**2\r\n",
        "  return square_diffs.mean()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3DwhKycLGc2"
      },
      "source": [
        "# 3. training with autograd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBy9OkIrLkVT"
      },
      "source": [
        "# params = [w, b]\r\n",
        "# requires_grad means we want pytorch to help us calculate\r\n",
        "# gradient on this tensor later.\r\n",
        "params = torch.tensor([1.0, 1.0], requires_grad = True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2cS2pB3M0so"
      },
      "source": [
        "# gradient calculation in action\r\n",
        "y_pred = model(tensor_celsius, *params)\r\n",
        "curr_loss = loss(y_pred, tensor_fahrenheit)\r\n",
        "curr_loss.backward()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lqr91eftNQoW",
        "outputId": "c15a5f1d-2602-4537-dddf-5b4d9bcd5247"
      },
      "source": [
        "# WOA LA, this is the gradient!\r\n",
        "params.grad"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-968.5273,  -80.6000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saMODB19NWrX",
        "outputId": "2422ee45-17d2-4300-f8c6-84684dbf6ab3"
      },
      "source": [
        "# zeroing out gradient (in place)\r\n",
        "params.grad.zero_()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0NAEExEK6Nm"
      },
      "source": [
        "def training_loop(num_epochs, learning_rate, params, x, y):\r\n",
        "  # zeroing out the gradient on this tensor!!!! very important\r\n",
        "  # because by default pytorch accumulate gradient, not set gradient\r\n",
        "  for curr_epoch in range(1, num_epochs + 1):\r\n",
        "    if params.grad is not None:\r\n",
        "      params.grad.zero_()\r\n",
        "    y_pred = model(x, *params)\r\n",
        "    curr_loss = loss(y_pred, y)\r\n",
        "    curr_loss.backward()\r\n",
        "    # with enters a new context environment when\r\n",
        "    # torch doesn't keep gradients of things, and so\r\n",
        "    # woudln't construct a new graph structure when we\r\n",
        "    # do calculatoin on the parameters.\r\n",
        "    with torch.no_grad():\r\n",
        "      params -= learning_rate * params.grad\r\n",
        "    if curr_epoch % 1000 == 0:\r\n",
        "      print(f'at epoch {curr_epoch} in the training, loss is {curr_loss}, and gradient is {params.grad}, and parameters is {params}')\r\n",
        "  return params"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPBqP2RnOu9K",
        "outputId": "f9a672ce-fa49-4dd4-e2c6-1832e3f96cde"
      },
      "source": [
        "final_params = training_loop(6000, 1e-3, torch.tensor([1., 1.], requires_grad=True), tensor_celsius, tensor_fahrenheit)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "at epoch 1000 in the training, loss is 90.67216491699219, and gradient is tensor([  0.6461, -11.5051]), and parameters is tensor([ 2.5789, 18.9740], requires_grad=True)\n",
            "at epoch 2000 in the training, loss is 25.434995651245117, and gradient is tensor([ 0.2843, -5.0619]), and parameters is tensor([ 2.1384, 26.8183], requires_grad=True)\n",
            "at epoch 3000 in the training, loss is 12.806656837463379, and gradient is tensor([ 0.1251, -2.2271]), and parameters is tensor([ 1.9445, 30.2697], requires_grad=True)\n",
            "at epoch 4000 in the training, loss is 10.36217975616455, and gradient is tensor([ 0.0550, -0.9799]), and parameters is tensor([ 1.8593, 31.7881], requires_grad=True)\n",
            "at epoch 5000 in the training, loss is 9.888985633850098, and gradient is tensor([ 0.0241, -0.4311]), and parameters is tensor([ 1.8218, 32.4562], requires_grad=True)\n",
            "at epoch 6000 in the training, loss is 9.797386169433594, and gradient is tensor([ 0.0107, -0.1897]), and parameters is tensor([ 1.8052, 32.7501], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6RB59qIO5sv",
        "outputId": "de1ed614-88df-40da-b14a-0d559f743319"
      },
      "source": [
        "final_params"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.8052, 32.7501], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMKdDDcgPPMd"
      },
      "source": [
        "As we can see, this is the correct `w` and `b` we have calculated before. This makes us more confident, as what we implemented matches with what pytorch autograd is doing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSY2ZoAOPeMP"
      },
      "source": [
        "# 4. using `Pytorch` `optim` package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43Fah-ttQFfc"
      },
      "source": [
        "We can accomplish the same thing in the above code using pytorch's optimization package. This optimization package contains a lot of more advanced optimizers like Adam. what we implemented above is vanilla SGD with full dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoHemNtoPeld"
      },
      "source": [
        "import torch.optim as optim"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woSh0S8cQZ-d",
        "outputId": "9c55b8cc-2fdf-4752-af8e-8b1debdbd372"
      },
      "source": [
        "# dir method gives all the attributes/functions of an object\r\n",
        "dir(optim)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ASGD',\n",
              " 'Adadelta',\n",
              " 'Adagrad',\n",
              " 'Adam',\n",
              " 'AdamW',\n",
              " 'Adamax',\n",
              " 'LBFGS',\n",
              " 'Optimizer',\n",
              " 'RMSprop',\n",
              " 'Rprop',\n",
              " 'SGD',\n",
              " 'SparseAdam',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '_multi_tensor',\n",
              " 'functional',\n",
              " 'lr_scheduler',\n",
              " 'swa_utils']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPGAHZ5vQq6q"
      },
      "source": [
        "optimizers have two main methods: `zero_grad()` and `step()`. `zero_grad()` zeros out the gradient accumulated at the last step on the parameters, as expected, `step()` update the parameters the optimizers own in the direction of the graident, with some added abilities if optimizers is more complicated. For example, `Adam` optimizer will adaptively set the learning rate. Also `Adam` is insensitive to the scale of parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hdpku_RaQhmP"
      },
      "source": [
        "def training_loop(num_epochs, optimizer, params, x, y):\r\n",
        "  for curr_epoch in range(1, num_epochs + 1):\r\n",
        "    # zero out gradient accumulated in previous step as always!\r\n",
        "    # this statement can be anywhere before .backward().\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    y_pred = model(x, *params)\r\n",
        "    curr_loss = loss(y_pred, y)\r\n",
        "    curr_loss.backward()\r\n",
        "\r\n",
        "    optimizer.step() # update parametes this optimizer owns via GD\r\n",
        "\r\n",
        "    if curr_epoch % 1000 == 0:\r\n",
        "      print(f'at epoch {curr_epoch} in the training, loss is {curr_loss}, and gradient is {params.grad}, and parameters is {params}')\r\n",
        "  return params"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7D9GNbER55N"
      },
      "source": [
        "params = torch.tensor([1., 1.], requires_grad=True)\r\n",
        "# constructor of optimizer takes in a list of list of parameters.\r\n",
        "SGD_optimizer = optim.SGD([params], lr = 1e-03)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WihFHw0nTB_y",
        "outputId": "1e83772f-16a2-4ebf-b1d2-b4ba72b04d32"
      },
      "source": [
        "training_loop(6000, SGD_optimizer, params, tensor_celsius, tensor_fahrenheit)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "at epoch 1000 in the training, loss is 90.67216491699219, and gradient is tensor([  0.6461, -11.5051]), and parameters is tensor([ 2.5789, 18.9740], requires_grad=True)\n",
            "at epoch 2000 in the training, loss is 25.434995651245117, and gradient is tensor([ 0.2843, -5.0619]), and parameters is tensor([ 2.1384, 26.8183], requires_grad=True)\n",
            "at epoch 3000 in the training, loss is 12.806656837463379, and gradient is tensor([ 0.1251, -2.2271]), and parameters is tensor([ 1.9445, 30.2697], requires_grad=True)\n",
            "at epoch 4000 in the training, loss is 10.36217975616455, and gradient is tensor([ 0.0550, -0.9799]), and parameters is tensor([ 1.8593, 31.7881], requires_grad=True)\n",
            "at epoch 5000 in the training, loss is 9.888985633850098, and gradient is tensor([ 0.0241, -0.4311]), and parameters is tensor([ 1.8218, 32.4562], requires_grad=True)\n",
            "at epoch 6000 in the training, loss is 9.797386169433594, and gradient is tensor([ 0.0107, -0.1897]), and parameters is tensor([ 1.8052, 32.7501], requires_grad=True)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.8052, 32.7501], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRkc16DTTXsD"
      },
      "source": [
        "this result is exactly like before, but now we are using optimizer's SGD update instead of our own!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yuInG6pTmGM"
      },
      "source": [
        "Let's try Adam, which is not sensitive to scale and sets adaptive learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb4YGSL6TVFI"
      },
      "source": [
        "params = torch.tensor([1., 1.], requires_grad=True)\r\n",
        "Adam_optimizer = optim.Adam([params])"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URtjF_TxTv5c",
        "outputId": "72bb2409-dd2e-48d1-c2eb-9168ef7371d9"
      },
      "source": [
        "training_loop(30000, Adam_optimizer, params, tensor_celsius, tensor_fahrenheit)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "at epoch 1000 in the training, loss is 903.4595947265625, and gradient is tensor([-610.3125,  -59.7573]), and parameters is tensor([1.9042, 1.9368], requires_grad=True)\n",
            "at epoch 2000 in the training, loss is 529.435546875, and gradient is tensor([-328.2633,  -43.2724]), and parameters is tensor([2.6102, 2.7633], requires_grad=True)\n",
            "at epoch 3000 in the training, loss is 389.9152526855469, and gradient is tensor([-130.3012,  -31.5630]), and parameters is tensor([3.0962, 3.5124], requires_grad=True)\n",
            "at epoch 4000 in the training, loss is 350.7929382324219, and gradient is tensor([-25.8822, -25.1154]), and parameters is tensor([3.3340, 4.2371], requires_grad=True)\n",
            "at epoch 5000 in the training, loss is 331.8125915527344, and gradient is tensor([  3.4438, -22.8338]), and parameters is tensor([3.3687, 5.0126], requires_grad=True)\n",
            "at epoch 6000 in the training, loss is 312.2503967285156, and gradient is tensor([  4.4817, -22.0649]), and parameters is tensor([3.3230, 5.8760], requires_grad=True)\n",
            "at epoch 7000 in the training, loss is 291.9792175292969, and gradient is tensor([  2.7762, -21.4002]), and parameters is tensor([3.2667, 6.7994], requires_grad=True)\n",
            "at epoch 8000 in the training, loss is 271.8831787109375, and gradient is tensor([  1.6698, -20.6808]), and parameters is tensor([3.2106, 7.7486], requires_grad=True)\n",
            "at epoch 9000 in the training, loss is 252.3571319580078, and gradient is tensor([  1.0013, -19.9296]), and parameters is tensor([3.1551, 8.7066], requires_grad=True)\n",
            "at epoch 10000 in the training, loss is 233.5461883544922, and gradient is tensor([  0.6028, -19.1614]), and parameters is tensor([3.1003, 9.6667], requires_grad=True)\n",
            "at epoch 11000 in the training, loss is 215.50990295410156, and gradient is tensor([  0.3628, -18.3850]), and parameters is tensor([ 3.0459, 10.6261], requires_grad=True)\n",
            "at epoch 12000 in the training, loss is 198.25608825683594, and gradient is tensor([  0.2194, -17.6044]), and parameters is tensor([ 2.9918, 11.5840], requires_grad=True)\n",
            "at epoch 13000 in the training, loss is 181.78948974609375, and gradient is tensor([  0.1326, -16.8221]), and parameters is tensor([ 2.9380, 12.5401], requires_grad=True)\n",
            "at epoch 14000 in the training, loss is 166.10916137695312, and gradient is tensor([  0.0802, -16.0397]), and parameters is tensor([ 2.8844, 13.4941], requires_grad=True)\n",
            "at epoch 15000 in the training, loss is 151.21157836914062, and gradient is tensor([  0.0484, -15.2579]), and parameters is tensor([ 2.8310, 14.4459], requires_grad=True)\n",
            "at epoch 16000 in the training, loss is 137.09495544433594, and gradient is tensor([  0.0293, -14.4773]), and parameters is tensor([ 2.7778, 15.3952], requires_grad=True)\n",
            "at epoch 17000 in the training, loss is 123.75513458251953, and gradient is tensor([  0.0178, -13.6985]), and parameters is tensor([ 2.7247, 16.3420], requires_grad=True)\n",
            "at epoch 18000 in the training, loss is 111.19318389892578, and gradient is tensor([ 1.0887e-02, -1.2922e+01]), and parameters is tensor([ 2.6718, 17.2857], requires_grad=True)\n",
            "at epoch 19000 in the training, loss is 99.40176391601562, and gradient is tensor([ 6.4316e-03, -1.2148e+01]), and parameters is tensor([ 2.6191, 18.2264], requires_grad=True)\n",
            "at epoch 20000 in the training, loss is 88.37979125976562, and gradient is tensor([ 3.8652e-03, -1.1376e+01]), and parameters is tensor([ 2.5665, 19.1634], requires_grad=True)\n",
            "at epoch 21000 in the training, loss is 78.12353515625, and gradient is tensor([ 2.4948e-03, -1.0608e+01]), and parameters is tensor([ 2.5143, 20.0964], requires_grad=True)\n",
            "at epoch 22000 in the training, loss is 68.62734985351562, and gradient is tensor([ 1.5116e-03, -9.8439e+00]), and parameters is tensor([ 2.4622, 21.0250], requires_grad=True)\n",
            "at epoch 23000 in the training, loss is 59.888851165771484, and gradient is tensor([ 8.9836e-04, -9.0838e+00]), and parameters is tensor([ 2.4105, 21.9484], requires_grad=True)\n",
            "at epoch 24000 in the training, loss is 51.90119934082031, and gradient is tensor([ 9.1314e-04, -8.3284e+00]), and parameters is tensor([ 2.3591, 22.8659], requires_grad=True)\n",
            "at epoch 25000 in the training, loss is 44.6581916809082, and gradient is tensor([ 2.8715e-03, -7.5786e+00]), and parameters is tensor([ 2.3080, 23.7765], requires_grad=True)\n",
            "at epoch 26000 in the training, loss is 38.153114318847656, and gradient is tensor([ 3.4714e-04, -6.8356e+00]), and parameters is tensor([ 2.2575, 24.6791], requires_grad=True)\n",
            "at epoch 27000 in the training, loss is 32.376224517822266, and gradient is tensor([ 1.2672e-03, -6.1003e+00]), and parameters is tensor([ 2.2074, 25.5722], requires_grad=True)\n",
            "at epoch 28000 in the training, loss is 27.317781448364258, and gradient is tensor([-0.0178, -5.3755]), and parameters is tensor([ 2.1581, 26.4539], requires_grad=True)\n",
            "at epoch 29000 in the training, loss is 22.964143753051758, and gradient is tensor([ 1.1132e-03, -4.6600e+00]), and parameters is tensor([ 2.1094, 27.3216], requires_grad=True)\n",
            "at epoch 30000 in the training, loss is 19.299131393432617, and gradient is tensor([-0.0110, -3.9606]), and parameters is tensor([ 2.0618, 28.1719], requires_grad=True)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 2.0618, 28.1719], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hBJtIbEUSPp"
      },
      "source": [
        "As we can see, Adam takes a lot of epoches to get to the optimum, but we don't need to set our own learning rate, which is cool!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvc72bHIUamo"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41rXRYnTUcZB"
      },
      "source": [
        "As we can see there are two things we need to keep in mind when we use `autograd` in `pytorch`:\r\n",
        "- remember to zero out gradient before next `.backward()` because `.backward()` **accumulates** the gradient on tensors, and so will add the current gradient to the gradient before it.\r\n",
        "- do `with torch.no_grad()` when update `parameters` as to not create a new computational graph out of it when we do calculation on a tensor that `requires_grad`. which it normally will do.\r\n",
        "- `optim` package abstracts away the optimization step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV_woaAUVGyU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}